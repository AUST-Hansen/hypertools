{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `reduce` function reduces the dimensionality of an array or list of arrays. The default reduction method is Principal Component Analysis, with a variety of other models supported.\n",
    "\n",
    "Supported models include: PCA, IncrementalPCA, SparsePCA, MiniBatchSparsePCA, KernelPCA, FastICA, FactorAnalysis, TruncatedSVD, DictionaryLearning, MiniBatchDictionaryLearning, TSNE, Isomap, SpectralEmbedding, LocallyLinearEmbedding, and MDS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Hypertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hypertools as hyp\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load your data\n",
    "\n",
    "First, we'll load one of the sample datasets. This dataset is a list of 2 `numpy` arrays, each containing average brain activity (fMRI) from 18 subjects listening to the same story, fit using Hierarchical Topographic Factor Analysis (HTFA) with 100 nodes.  The rows are timepoints and the columns are fMRI components. \n",
    "\n",
    "See the [full dataset](http://dataspace.princeton.edu/jspui/handle/88435/dsp015d86p269k) or the [Brainiak toolbox](www.brainiak.org) for more info on the data and HTFA, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = hyp.load('weights_avg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce one array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at one array from the dataset above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array shape: (300, 100)\n"
     ]
    }
   ],
   "source": [
    "print('Array shape: (%d, %d)' % weights[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce this array, simply pass the array to `hyp.reduce`, as below. We can see that the data has been reduced from 100 features to 3 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced array shape: (300, 3)\n"
     ]
    }
   ],
   "source": [
    "reduced_array = hyp.reduce(weights[0])\n",
    "print('Reduced array shape: (%d, %d)' % reduced_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce list of arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list or numpy array of multiple arrays can also be reduced by simply passing to hyp.reduce. Here we show this with two arrays in the weights dataset. First, let's examine the arrays in the weights dataset (below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's reduce both arrays at once (by passing in the whole of the weights data) and re-examine the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of first reduced array:  (300, 3)\n",
      "Shape of second reduced array:  (300, 3)\n"
     ]
    }
   ],
   "source": [
    "reduced_arrays = hyp.reduce(weights)\n",
    "print('Shape of first reduced array: ', reduced_arrays[0].shape)\n",
    "print('Shape of second reduced array: ', reduced_arrays[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that each array has been reduced from 100 features to 3 features, with the number of datapoints unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce list of arrays (TSNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also opt to use different reduction methods.  In the example below, we reduce multiple arrays at once, using TSNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of first reduced array:  (300, 3)\n",
      "Shape of second reduced array:  (300, 3)\n"
     ]
    }
   ],
   "source": [
    "reduced_TSNE = hyp.reduce(weights, reduce='TSNE')\n",
    "print('Shape of first reduced array: ',reduced_TSNE[0].shape)\n",
    "print('Shape of second reduced array: ',reduced_TSNE[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce to specified number of dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may prefer to reduce to a specific number of features, rather than defaulting the three dimensions.  To achieve this, simply pass the number of desired features (as an int) to the ndims argument, as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of first reduced array:  (300, 4)\n",
      "Shape of second reduced array:  (300, 4)\n"
     ]
    }
   ],
   "source": [
    "reduced_4 = hyp.reduce(weights, ndims = 4)\n",
    "print('Shape of first reduced array: ', reduced_4[0].shape)\n",
    "print('Shape of second reduced array: ', reduced_4[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce list of arrays with specific parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For finer control of parameters, a dictionary of model parameters may be passed to the reduce argument, in addition to the desired reduction method. See scikit-learn specific model docs for details on parameters supported for each model.\n",
    "\n",
    "Supported models include: PCA, IncrementalPCA, SparsePCA, MiniBatchSparsePCA, KernelPCA, FastICA, FactorAnalysis, TruncatedSVD, DictionaryLearning, MiniBatchDictionaryLearning, TSNE, Isomap, SpectralEmbedding, LocallyLinearEmbedding, and MDS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_params = hyp.reduce(weights, reduce={'model' : 'PCA', 'params' : {'whiten' : True}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
